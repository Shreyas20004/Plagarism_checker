title consistency group lasso multiple kernel learning authors francis bach abstract consider least square regression problem regularization block norm e sum euclidean norms spaces dimensions larger one problem referred group lasso extends usual regularization norm spaces dimension one commonly referred lasso paper study asymptotic model consistency group lasso derive necessary sufficient conditions consistency group lasso practical assumptions model misspecification linear predictors euclidean norms replaced functions reproducing kernel hilbert norms problem usually referred multiple kernel learning commonly used learning heterogeneous data sources non linear variable selection using tools functional analysis particular covariance operators extend consistency results infinite dimensional case also propose adaptive scheme obtain consistent model estimate even necessary condition required non adaptive scheme satisfied