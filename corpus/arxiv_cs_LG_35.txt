title general discounting versus average reward authors marcus hutter abstract consider agent interacting environment cycles every interaction cycle agent rewarded performance compare average reward u cycle average value future discounted reward v cycle k infinity discounted value consider essentially arbitrary non geometric discount sequences arbitrary reward sequences non mdp environments show asymptotically u infinity v k infinity equal provided limits exist effective horizon grows linearly k faster existence limit u implies limit v exists conversely effective horizon grows linearly k slower existence limit v implies limit u exists