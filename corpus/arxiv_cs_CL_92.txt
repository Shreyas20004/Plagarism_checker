title entropy based pruning backoff language models authors stolcke abstract criterion pruning parameters n gram backoff language models developed based relative entropy original pruned model shown relative entropy resulting pruning single n gram computed exactly efficiently backoff models relative entropy measure expressed relative change training set perplexity leads simple pruning criterion whereby n grams change perplexity less threshold removed model experiments show production quality hub lm reduced original size without increasing recognition error also compare approach heuristic pruning criterion seymore rosenfeld show approach interpreted approximation relative entropy criterion experimentally approaches select similar sets n grams overlap exact relative entropy criterion giving marginally better performance