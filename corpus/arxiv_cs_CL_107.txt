title tagger evaluation given hierarchical tag sets authors dan melamed philip resnik abstract present methods evaluating human automatic taggers extend current practice three ways first show evaluate taggers assign multiple tags test instance even assign probabilities second show accommodate common property manually constructed gold standards typically used objective evaluation namely often one correct answer third show measure performance set possible tags tree structured hierarchy illustrate methods used measure inter annotator agreement show compute kappa coefficient hierarchical tag sets