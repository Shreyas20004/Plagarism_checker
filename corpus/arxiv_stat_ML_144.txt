title fast learning rate lp mkl minimax optimality authors taiji suzuki abstract paper give new sharp generalization bound lp mkl generalized framework multiple kernel learning mkl imposes lp mixed norm regularization instead l mixed norm regularization utilize localization techniques obtain sharp learning rate bound characterized decay rate eigenvalues associated kernels larger decay rate gives faster convergence rate furthermore give minimax learning rate ball characterized lp mixed norm product space show derived learning rate lp mkl achieves minimax optimal rate lp mixed norm ball