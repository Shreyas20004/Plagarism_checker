title sharp convergence rate support consistency multiple kernel learning sparse dense regularization authors taiji suzuki ryota tomioka masashi sugiyama abstract theoretically investigate convergence rate support consistency e correctly identifying subset non zero coefficients large sample limit multiple kernel learning mkl focus mkl block l regularization inducing sparse kernel combination block l regularization inducing uniform kernel combination elastic net regularization including block l block l regularization case true kernel combination sparse show sharper convergence rate block l elastic net mkl methods existing rate block l mkl show elastic net mkl requires milder condition consistent block l mkl case optimal kernel combination exactly sparse prove elastic net mkl achieve faster convergence rate block l block l mkl methods carefully controlling balance block l block l regularizers thus theoretical results overall suggest use elastic net regularization mkl